{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Billboards, is a website that contains list ranking for a certain week.\n",
    "As an example, we will scrape the top 200 songs of the week, from it's relevant page.\n",
    "\n",
    "To demostrate how to scrape data from a website, we will scrape the top 200 songs of the week from the website https://www.billboard.com/charts/billboard-200/. The script bellow, downloads the webpage using the `requests` library. From there the html text is passed to `beutifulsoup` for parsing. After we have  pointer to the head Tag, we extract the rows, an finally we extract the elements per row.\n",
    "\n",
    "We use pydantic library to define a `schema` of how the data should look like, and using its functionality that make sure that the data are of that type. Aditional features are that it can validate the data, and convert it to json; which we are using later to create our output file.\n",
    "\n",
    "\n",
    "if you are missing a library needed, you can install the project dependencies using `pip install -r requirements.txt` or `poetry install --no-root --only=main` if you are using `poetry`. \n",
    "\n",
    "> Note: Pydantic is a validation library, focused on loading the data into a model that makes sure that is stronlgly typed. I absolutely recommend it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import bs4\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError\n",
    "from requests import get\n",
    "\n",
    "TARGET_URL = 'https://www.billboard.com/charts/billboard-200/'\n",
    "\n",
    "\n",
    "class BillBoardItem(BaseModel):\n",
    "    # scraped_timestamp is of type datetime. if not suplied, it will be set by default to datetime.now()\n",
    "    scraped_timestamp: datetime = datetime.now()\n",
    "    this_week_rank: int\n",
    "    # url is of type HttpUrl, which is a str with a valid URL.\n",
    "    url_photo: HttpUrl\n",
    "    artist: str\n",
    "    track_name: str\n",
    "    last_week_rank: str\n",
    "    peak_position: int\n",
    "    weeks_on_chart: str\n",
    "\n",
    "\n",
    "def get_parser(text) -> bs4.BeautifulSoup:\n",
    "    parser = bs4.BeautifulSoup(\n",
    "        markup=text,\n",
    "        features='html.parser'\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def parse_result(container_row: bs4.Tag) -> BillBoardItem:\n",
    "    last_week_rank,peak_position, weeks_on_chart =[t.text.strip() for t in container_row.select('li.o-chart-results-list__item > span.c-label')[-3:]]\n",
    "    this_week_rank, last_week_compare, track_name, artist_name = [t.text.strip() for t in container_row.select('li.o-chart-results-list__item span.c-label')[:4]]\n",
    "    image_urls = [img.attrs['data-lazy-src'] for img in container_row.select('img.c-lazy-image__img')]\n",
    "    track_name = container_row.select_one('#title-of-a-story').text.strip()\n",
    "    label= container_row.select_one('h3.c-title ~ p.c-tagline').text.strip\n",
    "    data = {\n",
    "        'this_week_rank': this_week_rank,\n",
    "        'url_photo': image_urls[-1],\n",
    "        'label': label,\n",
    "        'track_name': track_name,\n",
    "        'artist': artist_name,\n",
    "        'last_week_rank': last_week_rank,\n",
    "        'peak_position': peak_position,\n",
    "        'weeks_on_chart': weeks_on_chart\n",
    "        }\n",
    "    try:\n",
    "        return BillBoardItem(**data)\n",
    "    except ValidationError as e:\n",
    "        print(data)\n",
    "        raise e\n",
    "def parse_website(text: str) -> List[BillBoardItem]:\n",
    "    parser = get_parser(text)\n",
    "    items: List[BillBoardItem] = []\n",
    "    for idx, tag in enumerate(parser.find_all('div', class_={'o-chart-results-list-row-container'})):\n",
    "        items.append(parse_result(container_row=tag))\n",
    "    return items\n",
    "\n",
    "\n",
    "def main(**kwargs):\n",
    "    output = kwargs.get('output', sys.stdout)\n",
    "\n",
    "    r = get(url=TARGET_URL)\n",
    "    r.raise_for_status()\n",
    "    text = r.text\n",
    "    elements = parse_website(text)\n",
    "    for e in elements:\n",
    "        print(e.json(), file=output)\n",
    "\n",
    "\n",
    "with open('billboard200.jsonl', 'w') as f:\n",
    "    main(output=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "##### Grouping Key: \n",
    "Every week the entries are moving (or added/removed) according to their popularity.\n",
    "Create key based on the data to track their movement over the weeks.\n",
    " - Modify the BillBoardItem to include an id field, that auto-populates based on the data\n",
    "     - and read about validators (https://docs.pydantic.dev/usage/validators/) on how to autopopulate it\n",
    "\n",
    "##### Add awards to the data:\n",
    "- Modify the BillBoardItem to include an awards field, that auto-populates based on the data\n",
    "- tip: start with a sample row, and then try to extract the awards from it. \n",
    "    - Use the following code get started.\n",
    "    - get a list of all the rows, and then use the website to determine which row has awards on it\n",
    "    - then bit by bit try to extract the awards fields\n",
    "    - after you're successful, modify the code to add the awards to the BillBoardItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import bs4\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError\n",
    "from requests import get\n",
    "\n",
    "TARGET_URL = 'https://www.billboard.com/charts/billboard-200/'\n",
    "def get_parser(text) -> bs4.BeautifulSoup:\n",
    "    parser = bs4.BeautifulSoup(\n",
    "        markup=text,\n",
    "        features='html.parser'\n",
    "    )\n",
    "    return parser\n",
    "r = get(url=TARGET_URL)\n",
    "r.raise_for_status()\n",
    "text = r.text\n",
    "parser = get_parser(text)\n",
    "rows = parser.find_all('div', class_={'o-chart-results-list-row-container'})\n",
    "# which row has awards so you can work with it? chekc the site\n",
    "row = rows[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beutiful-soup-extraction-JlKjkdHa-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "507b8d1113d7bac468a8ac185a779936cc28fae36b75db0cd4039b8ff29d133e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
